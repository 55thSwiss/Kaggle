{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.sciencedirect.com/science/article/pii/S2211812814005318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib._GeneratorContextManager at 0x4f920278>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dependencies\n",
    "%matplotlib inline\n",
    "# import keras\n",
    "from keras.models import Sequential\n",
    "from keras.activations import *\n",
    "from keras.layers import *\n",
    "from keras.losses import *\n",
    "from keras.optimizers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import talos as ta\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# suppres NumPy arrays scientific notation and round decimals to three places\n",
    "np.set_printoptions(suppress=True)\n",
    "np.printoptions(precision=3, suppress=True)\n",
    "\n",
    "#libraries = [np,\n",
    "#             pd,\n",
    "#             sklearn,\n",
    "#             keras,\n",
    "#            ta]\n",
    "#for library in libraries:\n",
    "#    print(library, ' ', library.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data\n",
    "train_data = pd.read_csv('Training_Features_and_Labels.txt', delim_whitespace=True, encoding='ISO-8859-1')\n",
    "train_data.drop(columns=['Sr._No.'], inplace=True)\n",
    "# import validation data\n",
    "test_data = pd.read_csv('Test_Features_and_Labels.txt', delim_whitespace=True, encoding='ISO-8859-1')\n",
    "test_data.drop(columns=['Sr._No.'], inplace=True)\n",
    "# segregate a single row for validation\n",
    "validation_data = test_data.loc[0]\n",
    "test_data = test_data.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spindle_speed(rpm)    1100.00\n",
       "Feed_rate(mm/rev)        0.05\n",
       "Depth_of_cut(mm)         0.40\n",
       "MRR(mm3/min)          1106.51\n",
       "Ra(µm)                   0.21\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spindle_speed(rpm)</th>\n",
       "      <th>Feed_rate(mm/rev)</th>\n",
       "      <th>Depth_of_cut(mm)</th>\n",
       "      <th>MRR(mm3/min)</th>\n",
       "      <th>Ra(µm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1100</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.69</td>\n",
       "      <td>2020.44</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1095</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.20</td>\n",
       "      <td>6054.12</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1097</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.14</td>\n",
       "      <td>5329.21</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1094</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.15</td>\n",
       "      <td>5197.29</td>\n",
       "      <td>1.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Spindle_speed(rpm)  Feed_rate(mm/rev)  Depth_of_cut(mm)  MRR(mm3/min)  \\\n",
       "1                1100               0.05              0.69       2020.44   \n",
       "2                1095               0.15              1.20       6054.12   \n",
       "3                1097               0.13              1.14       5329.21   \n",
       "4                1094               0.12              1.15       5197.29   \n",
       "\n",
       "   Ra(µm)  \n",
       "1    0.29  \n",
       "2    1.87  \n",
       "3    1.54  \n",
       "4    1.45  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a look at the overall correlation between training and testing data\n",
    "df = pd.concat([train_data, test_data], axis=0, ignore_index=False)\n",
    "heat_data = df\n",
    "plt.figure(figsize=(16, 12))\n",
    "corr = heat_data.corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True,\n",
    "    annot=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=90,\n",
    "    horizontalalignment='right'\n",
    ");\n",
    "plt.savefig('heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      "\n",
      " [[ 306.67    0.36]\n",
      " [ 609.76    0.47]\n",
      " [ 909.28    0.52]\n",
      " [ 582.94    0.8 ]\n",
      " [1158.73    0.9 ]\n",
      " [1727.36    1.1 ]\n",
      " [ 943.34    1.63]\n",
      " [1875.96    1.76]\n",
      " [2797.84    2.17]\n",
      " [ 793.04    0.38]\n",
      " [1577.02    0.48]\n",
      " [2351.92    0.54]\n",
      " [1555.25    0.81]\n",
      " [3092.37    0.82]\n",
      " [4611.35    0.97]\n",
      " [2196.85    1.92]\n",
      " [4366.5     1.91]\n",
      " [6508.93    2.03]\n",
      " [ 875.13    0.29]\n",
      " [1735.95    0.37]\n",
      " [2582.46    0.38]\n",
      " [1745.24    0.82]\n",
      " [3461.88    0.79]\n",
      " [5149.9     1.11]\n",
      " [2549.2     1.75]\n",
      " [5055.49    1.89]\n",
      " [7518.86    1.82]]\n",
      "\n",
      "Training Labels:\n",
      "\n",
      " [[ 280.        0.0508    0.4   ]\n",
      " [ 280.        0.0508    0.8   ]\n",
      " [ 280.        0.0508    1.2   ]\n",
      " [ 280.        0.1016    0.4   ]\n",
      " [ 280.        0.1016    0.8   ]\n",
      " [ 280.        0.1016    1.2   ]\n",
      " [ 280.        0.1524    0.4   ]\n",
      " [ 280.        0.1524    0.8   ]\n",
      " [ 280.        0.1524    1.2   ]\n",
      " [ 710.        0.0508    0.4   ]\n",
      " [ 710.        0.0508    0.8   ]\n",
      " [ 710.        0.0508    1.2   ]\n",
      " [ 710.        0.1016    0.4   ]\n",
      " [ 710.        0.1016    0.8   ]\n",
      " [ 710.        0.1016    1.2   ]\n",
      " [ 710.        0.1524    0.4   ]\n",
      " [ 710.        0.1524    0.8   ]\n",
      " [ 710.        0.1524    1.2   ]\n",
      " [1120.        0.0508    0.4   ]\n",
      " [1120.        0.0508    0.8   ]\n",
      " [1120.        0.0508    1.2   ]\n",
      " [1120.        0.1016    0.4   ]\n",
      " [1120.        0.1016    0.8   ]\n",
      " [1120.        0.1016    1.2   ]\n",
      " [1120.        0.1524    0.4   ]\n",
      " [1120.        0.1524    0.8   ]\n",
      " [1120.        0.1524    1.2   ]]\n"
     ]
    }
   ],
   "source": [
    "# create NumPy arrays of training features and labels\n",
    "train_features = train_data[['MRR(mm3/min)', 'Ra(µm)']].values\n",
    "train_labels = train_data[['Spindle_speed(rpm)', 'Feed_rate(mm/rev)', 'Depth_of_cut(mm)']].values\n",
    "print('Training Features:\\n\\n', train_features)\n",
    "print('\\nTraining Labels:\\n\\n', train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Features:\n",
      "\n",
      " [[2020.44    0.29]\n",
      " [6054.12    1.87]\n",
      " [5329.21    1.54]\n",
      " [5197.29    1.45]]\n",
      "\n",
      "Testing Labels:\n",
      "\n",
      " [[1100.      0.05    0.69]\n",
      " [1095.      0.15    1.2 ]\n",
      " [1097.      0.13    1.14]\n",
      " [1094.      0.12    1.15]]\n"
     ]
    }
   ],
   "source": [
    "# create NumPy arrays of validation features and labels\n",
    "test_features = test_data[['MRR(mm3/min)', 'Ra(µm)']].values\n",
    "test_labels = test_data[['Spindle_speed(rpm)', 'Feed_rate(mm/rev)', 'Depth_of_cut(mm)']].values\n",
    "print('Testing Features:\\n\\n', test_features)\n",
    "print('\\nTesting Labels:\\n\\n', test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Features:\n",
      "\n",
      " [[1106.51    0.21]]\n",
      "\n",
      "Validation Labels:\n",
      "\n",
      " [[1100.      0.05    0.4 ]]\n"
     ]
    }
   ],
   "source": [
    "# create NumPy array for validation data\n",
    "validation_features = validation_data[['MRR(mm3/min)', 'Ra(µm)']].values\n",
    "validation_labels = validation_data[['Spindle_speed(rpm)', 'Feed_rate(mm/rev)', 'Depth_of_cut(mm)']].values\n",
    "\n",
    "# reshape the validation data because it's only one row\n",
    "validation_features = validation_features.reshape(1, -1)\n",
    "validation_labels = validation_labels.reshape(1, -1)\n",
    "\n",
    "print('Validation Features:\\n\\n', validation_features)\n",
    "print('\\nValidation Labels:\\n\\n', validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms features by scaling each feature to a given range, -1 and 1 in this case\n",
    "MinMax_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train_features_MinMax = MinMax_scaler.fit_transform(train_features)\n",
    "train_labels_MinMax = MinMax_scaler.fit_transform(train_labels)\n",
    "test_features_MinMax = MinMax_scaler.fit_transform(test_features)\n",
    "test_labels_MinMax = MinMax_scaler.fit_transform(test_labels)\n",
    "validation_features_MinMax = MinMax_scaler.fit_transform(validation_features)\n",
    "validation_labels_MinMax = MinMax_scaler.fit_transform(validation_labels)\n",
    "\n",
    "#MinMax_scaler.inverse_transform(validation_labels_MinMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spindle_speed(rpm)</th>\n",
       "      <th>Feed_rate(mm/rev)</th>\n",
       "      <th>Depth_of_cut(mm)</th>\n",
       "      <th>MRR(mm3/min)</th>\n",
       "      <th>Ra(µm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1100</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.69</td>\n",
       "      <td>2020.44</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1095</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.20</td>\n",
       "      <td>6054.12</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1097</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.14</td>\n",
       "      <td>5329.21</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1094</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.15</td>\n",
       "      <td>5197.29</td>\n",
       "      <td>1.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Spindle_speed(rpm)  Feed_rate(mm/rev)  Depth_of_cut(mm)  MRR(mm3/min)  \\\n",
       "1                1100               0.05              0.69       2020.44   \n",
       "2                1095               0.15              1.20       6054.12   \n",
       "3                1097               0.13              1.14       5329.21   \n",
       "4                1094               0.12              1.15       5197.29   \n",
       "\n",
       "   Ra(µm)  \n",
       "1    0.29  \n",
       "2    1.87  \n",
       "3    1.54  \n",
       "4    1.45  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1100.  ,    0.05,    0.69],\n",
       "       [1095.  ,    0.15,    1.2 ],\n",
       "       [1097.  ,    0.13,    1.14],\n",
       "       [1094.  ,    0.12,    1.15]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -1.        , -1.        ],\n",
       "       [-0.66666667,  1.        ,  1.        ],\n",
       "       [ 0.        ,  0.6       ,  0.76470588],\n",
       "       [-1.        ,  0.4       ,  0.80392157]])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1101.        ,    0.05      ,    0.4       ],\n",
       "       [1100.16666667,    1.05      ,    1.4       ],\n",
       "       [1100.5       ,    0.85      ,    1.28235294],\n",
       "       [1100.        ,    0.75      ,    1.30196078]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MinMax_scaler.inverse_transform(test_labels_MinMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "Standard_scaler = StandardScaler()\n",
    "train_features_Standard = Standard_scaler.fit_transform(train_features)\n",
    "train_labels_Standard = Standard_scaler.fit_transform(train_labels)\n",
    "test_features_Standard = Standard_scaler.fit_transform(test_features)\n",
    "test_labels_Standard = Standard_scaler.fit_transform(test_labels)\n",
    "validation_features_Standard = Standard_scaler.fit_transform(validation_features)\n",
    "validation_labels_Standard = Standard_scaler.fit_transform(validation_labels)\n",
    "\n",
    "# Standard_scaler.inverse_transform(train_features_Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron': [8, 12, 24],\n",
    "    'second_neuron': [36, 48, 62, 84],\n",
    "    'third_neuron': [36, 48, 62, 84],\n",
    "    'fourth_neuron': [8, 12, 24],\n",
    "    'batch_size': [10, 20, 30],\n",
    "    'activation': [relu,\n",
    "                   #softmax,\n",
    "                   #selu,\n",
    "                   #softplus,\n",
    "                   #softsign,\n",
    "                   #tanh,\n",
    "                   #sigmoid,\n",
    "                   #hard_sigmoid,\n",
    "                   #exponential,\n",
    "                   linear],\n",
    "    'optimizer' : [#'SGD',\n",
    "                   #'RMSprop',\n",
    "                   #'Adagrad',\n",
    "                   #'Adadelta',\n",
    "                   'Adam',],\n",
    "                   #'Adamax',\n",
    "                   #'Nadam',],\n",
    "    'loss' : [mean_squared_error,]\n",
    "              #mean_absolute_error,] \n",
    "              #mean_absolute_percentage_error, \n",
    "              #mean_squared_logarithmic_error,\n",
    "              #squared_hinge,hinge,\n",
    "              #categorical_hinge,\n",
    "              #logcosh,],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMax_ann(train_features, train_labels, test_features, test_labels, params):\n",
    "    \n",
    "    # replace the hyperparameter inputs with references to params dictionary \n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['first_neuron'], input_dim=2, activation=params['activation']))\n",
    "    model.add(Dense(params['second_neuron'], activation=params['activation']))\n",
    "    model.add(Dense(params['third_neuron'], activation=params['activation']))\n",
    "    model.add(Dense(params['fourth_neuron'], activation=params['activation']))\n",
    "    model.add(Dense(3, activation='linear'))\n",
    "    model.compile(loss=params['loss'], optimizer=params['optimizer'], metrics=['accuracy'])\n",
    "    \n",
    "    # make sure history object is returned by model.fit()\n",
    "    out = model.fit(train_features_MinMax, train_labels_MinMax,\n",
    "                    epochs=100,\n",
    "                    batch_size=params['batch_size'],\n",
    "                    verbose=0,\n",
    "                    validation_data=[test_features_MinMax, test_labels_MinMax])\n",
    "    \n",
    "    # model output\n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standard_ann(train_features, train_labels, test_features, test_labels, params):\n",
    "    \n",
    "    # replace the hyperparameter inputs with references to params dictionary \n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['first_neuron'], input_dim=2, activation=params['activation']))\n",
    "    model.add(Dense(params['second_neuron'], activation=params['activation']))\n",
    "    model.add(Dense(params['third_neuron'], activation=params['activation']))\n",
    "    model.add(Dense(params['fourth_neuron'], activation=params['activation']))\n",
    "    model.add(Dense(3, activation='linear'))\n",
    "    model.compile(loss=params['loss'], optimizer=params['optimizer'], metrics=['accuracy'])\n",
    "    \n",
    "    # make sure history object is returned by model.fit()\n",
    "    out = model.fit(train_features_Standard, train_labels_Standard,\n",
    "                    epochs=100,\n",
    "                    batch_size=params['batch_size'],\n",
    "                    verbose=0,\n",
    "                    validation_data=[test_features_Standard, test_labels_Standard])\n",
    "    \n",
    "    # model output\n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_MinMax = np.concatenate((train_features_MinMax, test_features_MinMax), axis=0)\n",
    "labels_MinMax = np.concatenate((train_labels_MinMax, test_labels_MinMax), axis=0)\n",
    "\n",
    "features_Standard = np.concatenate((train_features_Standard, test_features_Standard), axis=0)\n",
    "labels_Standard = np.concatenate((train_labels_Standard, test_labels_Standard), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the Talos experiment on MinMax data\n",
    "MinMax_t = ta.Scan(features_MinMax, labels_MinMax, \n",
    "            params=p, \n",
    "            model=MinMax_ann,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the Talos experiment on Standard data\n",
    "Standard_t = ta.Scan(features_Standard, labels_Standard, \n",
    "            params=p, \n",
    "            model=Standard_ann,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>fourth_neuron</th>\n",
       "      <th>loss.1</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>second_neuron</th>\n",
       "      <th>third_neuron</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.261867</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.275643</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>&lt;function relu at 0x000001AA17107D08&gt;</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;function mean_squared_error at 0x000001AA170D...</td>\n",
       "      <td>Adam</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0.277996</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.264963</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>&lt;function relu at 0x000001AA17107D08&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;function mean_squared_error at 0x000001AA170D...</td>\n",
       "      <td>Adam</td>\n",
       "      <td>36</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>0.302887</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.452316</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>&lt;function relu at 0x000001AA17107D08&gt;</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;function mean_squared_error at 0x000001AA170D...</td>\n",
       "      <td>Adam</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round_epochs  val_loss  val_acc      loss       acc  \\\n",
       "0           100  0.261867     0.75  0.275643  0.481481   \n",
       "1           100  0.277996     1.00  0.264963  0.518519   \n",
       "2           100  0.302887     0.25  0.452316  0.555556   \n",
       "\n",
       "                              activation  batch_size  first_neuron  \\\n",
       "0  <function relu at 0x000001AA17107D08>          30            24   \n",
       "1  <function relu at 0x000001AA17107D08>          10             8   \n",
       "2  <function relu at 0x000001AA17107D08>          30            12   \n",
       "\n",
       "   fourth_neuron                                             loss.1 optimizer  \\\n",
       "0              8  <function mean_squared_error at 0x000001AA170D...      Adam   \n",
       "1              8  <function mean_squared_error at 0x000001AA170D...      Adam   \n",
       "2              8  <function mean_squared_error at 0x000001AA170D...      Adam   \n",
       "\n",
       "   second_neuron  third_neuron  \n",
       "0             48            48  \n",
       "1             36            84  \n",
       "2             48            48  "
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('MinMax.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "27/27 [==============================] - 2s 75ms/step - loss: 0.6720 - acc: 0.2593\n",
      "Epoch 2/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.6638 - acc: 0.2593\n",
      "Epoch 3/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.6565 - acc: 0.2593\n",
      "Epoch 4/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6492 - acc: 0.2593\n",
      "Epoch 5/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6415 - acc: 0.2963\n",
      "Epoch 6/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.6339 - acc: 0.2963\n",
      "Epoch 7/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.6264 - acc: 0.3333\n",
      "Epoch 8/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6194 - acc: 0.3704\n",
      "Epoch 9/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.6128 - acc: 0.3333\n",
      "Epoch 10/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.6069 - acc: 0.3704\n",
      "Epoch 11/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.6012 - acc: 0.4074\n",
      "Epoch 12/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.5956 - acc: 0.4444\n",
      "Epoch 13/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5897 - acc: 0.3704\n",
      "Epoch 14/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5837 - acc: 0.3704\n",
      "Epoch 15/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.5777 - acc: 0.3704\n",
      "Epoch 16/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.5717 - acc: 0.3704\n",
      "Epoch 17/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.5658 - acc: 0.3333\n",
      "Epoch 18/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.5599 - acc: 0.3704\n",
      "Epoch 19/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.5539 - acc: 0.4444\n",
      "Epoch 20/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.5481 - acc: 0.4444\n",
      "Epoch 21/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5424 - acc: 0.5556\n",
      "Epoch 22/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5366 - acc: 0.5185\n",
      "Epoch 23/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.5308 - acc: 0.5185\n",
      "Epoch 24/200\n",
      "27/27 [==============================] - 0s 648us/step - loss: 0.5248 - acc: 0.5185\n",
      "Epoch 25/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.5188 - acc: 0.5185\n",
      "Epoch 26/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.5127 - acc: 0.5185\n",
      "Epoch 27/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.5065 - acc: 0.4815\n",
      "Epoch 28/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.5001 - acc: 0.4815\n",
      "Epoch 29/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.4936 - acc: 0.5185\n",
      "Epoch 30/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.4870 - acc: 0.5185\n",
      "Epoch 31/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4806 - acc: 0.5185\n",
      "Epoch 32/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4742 - acc: 0.5185\n",
      "Epoch 33/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.4679 - acc: 0.4815\n",
      "Epoch 34/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4615 - acc: 0.4815\n",
      "Epoch 35/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.4551 - acc: 0.4815\n",
      "Epoch 36/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.4487 - acc: 0.5185\n",
      "Epoch 37/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.4425 - acc: 0.5185\n",
      "Epoch 38/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.4365 - acc: 0.5185\n",
      "Epoch 39/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4307 - acc: 0.5185\n",
      "Epoch 40/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.4249 - acc: 0.5185\n",
      "Epoch 41/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.4191 - acc: 0.5185\n",
      "Epoch 42/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4134 - acc: 0.5185\n",
      "Epoch 43/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.4077 - acc: 0.5185\n",
      "Epoch 44/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4020 - acc: 0.5185\n",
      "Epoch 45/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.3965 - acc: 0.5556\n",
      "Epoch 46/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.3914 - acc: 0.5556\n",
      "Epoch 47/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.3865 - acc: 0.5556\n",
      "Epoch 48/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.3819 - acc: 0.5556\n",
      "Epoch 49/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.3775 - acc: 0.5556\n",
      "Epoch 50/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.3735 - acc: 0.5556\n",
      "Epoch 51/200\n",
      "27/27 [==============================] - 0s 648us/step - loss: 0.3697 - acc: 0.5556\n",
      "Epoch 52/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.3660 - acc: 0.5556\n",
      "Epoch 53/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.3626 - acc: 0.5556\n",
      "Epoch 54/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.3594 - acc: 0.5556\n",
      "Epoch 55/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.3563 - acc: 0.5556\n",
      "Epoch 56/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.3533 - acc: 0.5556\n",
      "Epoch 57/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.3504 - acc: 0.5556\n",
      "Epoch 58/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.3476 - acc: 0.5556\n",
      "Epoch 59/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.3449 - acc: 0.5556\n",
      "Epoch 60/200\n",
      "27/27 [==============================] - 0s 648us/step - loss: 0.3422 - acc: 0.5556\n",
      "Epoch 61/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.3395 - acc: 0.5556\n",
      "Epoch 62/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.3369 - acc: 0.5556\n",
      "Epoch 63/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.3343 - acc: 0.5556\n",
      "Epoch 64/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.3317 - acc: 0.5556\n",
      "Epoch 65/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.3291 - acc: 0.5556\n",
      "Epoch 66/200\n",
      "27/27 [==============================] - 0s 648us/step - loss: 0.3265 - acc: 0.5556\n",
      "Epoch 67/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.3239 - acc: 0.5556\n",
      "Epoch 68/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.3215 - acc: 0.5556\n",
      "Epoch 69/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.3189 - acc: 0.5556\n",
      "Epoch 70/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.3164 - acc: 0.5556\n",
      "Epoch 71/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.3140 - acc: 0.5556\n",
      "Epoch 72/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.3116 - acc: 0.5556\n",
      "Epoch 73/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.3091 - acc: 0.5556\n",
      "Epoch 74/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.3064 - acc: 0.5926\n",
      "Epoch 75/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.3028 - acc: 0.5926\n",
      "Epoch 76/200\n",
      "27/27 [==============================] - 0s 648us/step - loss: 0.2988 - acc: 0.5926\n",
      "Epoch 77/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2950 - acc: 0.5926\n",
      "Epoch 78/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2913 - acc: 0.5926\n",
      "Epoch 79/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2877 - acc: 0.5926\n",
      "Epoch 80/200\n",
      "27/27 [==============================] - 0s 648us/step - loss: 0.2843 - acc: 0.5926\n",
      "Epoch 81/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2808 - acc: 0.6296\n",
      "Epoch 82/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2774 - acc: 0.6296\n",
      "Epoch 83/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2742 - acc: 0.6296\n",
      "Epoch 84/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2711 - acc: 0.6296\n",
      "Epoch 85/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2681 - acc: 0.6296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2652 - acc: 0.6296\n",
      "Epoch 87/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.2626 - acc: 0.6296\n",
      "Epoch 88/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2602 - acc: 0.6296\n",
      "Epoch 89/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.2580 - acc: 0.6296\n",
      "Epoch 90/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.2560 - acc: 0.6296\n",
      "Epoch 91/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2540 - acc: 0.5926\n",
      "Epoch 92/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.2522 - acc: 0.5926\n",
      "Epoch 93/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2509 - acc: 0.5926\n",
      "Epoch 94/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.2498 - acc: 0.5926\n",
      "Epoch 95/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.2487 - acc: 0.5926\n",
      "Epoch 96/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2478 - acc: 0.5926\n",
      "Epoch 97/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2469 - acc: 0.6296\n",
      "Epoch 98/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.2461 - acc: 0.6296\n",
      "Epoch 99/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2453 - acc: 0.6296\n",
      "Epoch 100/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2445 - acc: 0.6296\n",
      "Epoch 101/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2437 - acc: 0.6296\n",
      "Epoch 102/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2429 - acc: 0.6296\n",
      "Epoch 103/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2421 - acc: 0.6296\n",
      "Epoch 104/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.2413 - acc: 0.6296\n",
      "Epoch 105/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2406 - acc: 0.6296\n",
      "Epoch 106/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.2399 - acc: 0.6296\n",
      "Epoch 107/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.2392 - acc: 0.6296\n",
      "Epoch 108/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.2385 - acc: 0.6296\n",
      "Epoch 109/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.2378 - acc: 0.6296\n",
      "Epoch 110/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2371 - acc: 0.6296\n",
      "Epoch 111/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2364 - acc: 0.6296\n",
      "Epoch 112/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.2356 - acc: 0.6296\n",
      "Epoch 113/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2349 - acc: 0.6296\n",
      "Epoch 114/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.2342 - acc: 0.6296\n",
      "Epoch 115/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.2335 - acc: 0.6296\n",
      "Epoch 116/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.2328 - acc: 0.6296\n",
      "Epoch 117/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.2320 - acc: 0.6296\n",
      "Epoch 118/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2314 - acc: 0.6296\n",
      "Epoch 119/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2307 - acc: 0.6296\n",
      "Epoch 120/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2300 - acc: 0.6296\n",
      "Epoch 121/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2293 - acc: 0.6296\n",
      "Epoch 122/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.2287 - acc: 0.6296\n",
      "Epoch 123/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.2280 - acc: 0.6296\n",
      "Epoch 124/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2275 - acc: 0.6296\n",
      "Epoch 125/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2269 - acc: 0.6296\n",
      "Epoch 126/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2262 - acc: 0.6296\n",
      "Epoch 127/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.2255 - acc: 0.6296\n",
      "Epoch 128/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2250 - acc: 0.6296\n",
      "Epoch 129/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2243 - acc: 0.6296\n",
      "Epoch 130/200\n",
      "27/27 [==============================] - 0s 648us/step - loss: 0.2237 - acc: 0.6296\n",
      "Epoch 131/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2229 - acc: 0.6296\n",
      "Epoch 132/200\n",
      "27/27 [==============================] - 0s 648us/step - loss: 0.2223 - acc: 0.6296\n",
      "Epoch 133/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2217 - acc: 0.6296\n",
      "Epoch 134/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2210 - acc: 0.6296\n",
      "Epoch 135/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2203 - acc: 0.6296\n",
      "Epoch 136/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2197 - acc: 0.6296\n",
      "Epoch 137/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2189 - acc: 0.6296\n",
      "Epoch 138/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2182 - acc: 0.6296\n",
      "Epoch 139/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2177 - acc: 0.5926\n",
      "Epoch 140/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2169 - acc: 0.5926\n",
      "Epoch 141/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2163 - acc: 0.5926\n",
      "Epoch 142/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2155 - acc: 0.5926\n",
      "Epoch 143/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2149 - acc: 0.5926\n",
      "Epoch 144/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2142 - acc: 0.5926\n",
      "Epoch 145/200\n",
      "27/27 [==============================] - 0s 648us/step - loss: 0.2135 - acc: 0.5926\n",
      "Epoch 146/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2128 - acc: 0.5926\n",
      "Epoch 147/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2121 - acc: 0.5926\n",
      "Epoch 148/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2114 - acc: 0.5926\n",
      "Epoch 149/200\n",
      "27/27 [==============================] - 0s 648us/step - loss: 0.2107 - acc: 0.5926\n",
      "Epoch 150/200\n",
      "27/27 [==============================] - 0s 648us/step - loss: 0.2100 - acc: 0.5926\n",
      "Epoch 151/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2093 - acc: 0.5926\n",
      "Epoch 152/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.2086 - acc: 0.5926\n",
      "Epoch 153/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2078 - acc: 0.5926\n",
      "Epoch 154/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.2071 - acc: 0.5926\n",
      "Epoch 155/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2063 - acc: 0.5926\n",
      "Epoch 156/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.2056 - acc: 0.5926\n",
      "Epoch 157/200\n",
      "27/27 [==============================] - 0s 648us/step - loss: 0.2047 - acc: 0.5926\n",
      "Epoch 158/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2041 - acc: 0.6296\n",
      "Epoch 159/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2033 - acc: 0.6667\n",
      "Epoch 160/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2025 - acc: 0.6667\n",
      "Epoch 161/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.2017 - acc: 0.6667\n",
      "Epoch 162/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2010 - acc: 0.6667\n",
      "Epoch 163/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.2002 - acc: 0.6667\n",
      "Epoch 164/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.1994 - acc: 0.6296\n",
      "Epoch 165/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.1986 - acc: 0.6296\n",
      "Epoch 166/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.1977 - acc: 0.6296\n",
      "Epoch 167/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.1970 - acc: 0.6667\n",
      "Epoch 168/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.1961 - acc: 0.6667\n",
      "Epoch 169/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.1953 - acc: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.1945 - acc: 0.6667\n",
      "Epoch 171/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.1937 - acc: 0.6667\n",
      "Epoch 172/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.1929 - acc: 0.6667\n",
      "Epoch 173/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.1920 - acc: 0.6667\n",
      "Epoch 174/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.1912 - acc: 0.6667\n",
      "Epoch 175/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.1905 - acc: 0.6667\n",
      "Epoch 176/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.1896 - acc: 0.6667\n",
      "Epoch 177/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.1888 - acc: 0.6667\n",
      "Epoch 178/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.1880 - acc: 0.6667\n",
      "Epoch 179/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.1871 - acc: 0.6667\n",
      "Epoch 180/200\n",
      "27/27 [==============================] - 0s 963us/step - loss: 0.1863 - acc: 0.6667\n",
      "Epoch 181/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.1856 - acc: 0.6667\n",
      "Epoch 182/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.1847 - acc: 0.6667\n",
      "Epoch 183/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.1839 - acc: 0.6667\n",
      "Epoch 184/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.1831 - acc: 0.6667\n",
      "Epoch 185/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.1823 - acc: 0.6667\n",
      "Epoch 186/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.1814 - acc: 0.6667\n",
      "Epoch 187/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.1806 - acc: 0.6667\n",
      "Epoch 188/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.1798 - acc: 0.6667\n",
      "Epoch 189/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.1790 - acc: 0.6667\n",
      "Epoch 190/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.1782 - acc: 0.6667\n",
      "Epoch 191/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.1773 - acc: 0.6667\n",
      "Epoch 192/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.1765 - acc: 0.6667\n",
      "Epoch 193/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.1757 - acc: 0.6667\n",
      "Epoch 194/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.1749 - acc: 0.6667\n",
      "Epoch 195/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.1741 - acc: 0.6667\n",
      "Epoch 196/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.1733 - acc: 0.6667\n",
      "Epoch 197/200\n",
      "27/27 [==============================] - 0s 741us/step - loss: 0.1725 - acc: 0.6667\n",
      "Epoch 198/200\n",
      "27/27 [==============================] - 0s 648us/step - loss: 0.1717 - acc: 0.6667\n",
      "Epoch 199/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.1708 - acc: 0.6667\n",
      "Epoch 200/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.1701 - acc: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x508afeb8>"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the MinMax model\n",
    "MinMax_model = Sequential()\n",
    "MinMax_model.add(Dense(8, input_dim=2, activation='relu'))\n",
    "MinMax_model.add(Dense(48, activation='relu'))\n",
    "MinMax_model.add(Dense(48, activation='relu'))\n",
    "MinMax_model.add(Dense(8, activation='relu'))\n",
    "MinMax_model.add(Dense(3, activation='linear'))\n",
    "MinMax_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "MinMax_model.fit(train_features_MinMax,\n",
    "                 train_labels_MinMax,\n",
    "                 #validation_data=(test_features_MinMax, test_labels_MinMax),\n",
    "                 batch_size=30,\n",
    "                 epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>fourth_neuron</th>\n",
       "      <th>loss.1</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>second_neuron</th>\n",
       "      <th>third_neuron</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.386672</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.459910</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>&lt;function relu at 0x000001AA17107D08&gt;</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;function mean_squared_error at 0x000001AA170D...</td>\n",
       "      <td>Adam</td>\n",
       "      <td>48</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0.466409</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.401966</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>&lt;function relu at 0x000001AA17107D08&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;function mean_squared_error at 0x000001AA170D...</td>\n",
       "      <td>Adam</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>0.511823</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.447284</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>&lt;function linear at 0x000001AA17118048&gt;</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>&lt;function mean_squared_error at 0x000001AA170D...</td>\n",
       "      <td>Adam</td>\n",
       "      <td>36</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round_epochs  val_loss  val_acc      loss       acc  \\\n",
       "0           100  0.386672     0.75  0.459910  0.666667   \n",
       "1           100  0.466409     0.75  0.401966  0.777778   \n",
       "2           100  0.511823     0.75  0.447284  0.629630   \n",
       "\n",
       "                                activation  batch_size  first_neuron  \\\n",
       "0    <function relu at 0x000001AA17107D08>          30            24   \n",
       "1    <function relu at 0x000001AA17107D08>          10            12   \n",
       "2  <function linear at 0x000001AA17118048>          20            24   \n",
       "\n",
       "   fourth_neuron                                             loss.1 optimizer  \\\n",
       "0              8  <function mean_squared_error at 0x000001AA170D...      Adam   \n",
       "1              8  <function mean_squared_error at 0x000001AA170D...      Adam   \n",
       "2             12  <function mean_squared_error at 0x000001AA170D...      Adam   \n",
       "\n",
       "   second_neuron  third_neuron  \n",
       "0             48            62  \n",
       "1             84            84  \n",
       "2             36            62  "
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Standard.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27 samples, validate on 4 samples\n",
      "Epoch 1/200\n",
      "27/27 [==============================] - 2s 81ms/step - loss: 0.9962 - acc: 0.4074 - val_loss: 0.9516 - val_acc: 0.5000\n",
      "Epoch 2/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.9857 - acc: 0.4074 - val_loss: 0.9425 - val_acc: 0.5000\n",
      "Epoch 3/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.9744 - acc: 0.4074 - val_loss: 0.9343 - val_acc: 0.5000\n",
      "Epoch 4/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.9626 - acc: 0.4815 - val_loss: 0.9277 - val_acc: 0.5000\n",
      "Epoch 5/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.9501 - acc: 0.4815 - val_loss: 0.9213 - val_acc: 0.5000\n",
      "Epoch 6/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.9379 - acc: 0.4444 - val_loss: 0.9142 - val_acc: 0.7500\n",
      "Epoch 7/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.9273 - acc: 0.4444 - val_loss: 0.9066 - val_acc: 0.7500\n",
      "Epoch 8/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.9164 - acc: 0.5185 - val_loss: 0.8999 - val_acc: 0.7500\n",
      "Epoch 9/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.9057 - acc: 0.5926 - val_loss: 0.8932 - val_acc: 0.7500\n",
      "Epoch 10/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.8951 - acc: 0.6296 - val_loss: 0.8867 - val_acc: 0.7500\n",
      "Epoch 11/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.8853 - acc: 0.5556 - val_loss: 0.8810 - val_acc: 0.5000\n",
      "Epoch 12/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.8758 - acc: 0.5556 - val_loss: 0.8757 - val_acc: 0.5000\n",
      "Epoch 13/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.8664 - acc: 0.5185 - val_loss: 0.8705 - val_acc: 0.2500\n",
      "Epoch 14/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.8572 - acc: 0.5185 - val_loss: 0.8652 - val_acc: 0.2500\n",
      "Epoch 15/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.8483 - acc: 0.4444 - val_loss: 0.8593 - val_acc: 0.2500\n",
      "Epoch 16/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.8397 - acc: 0.4444 - val_loss: 0.8531 - val_acc: 0.2500\n",
      "Epoch 17/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.8312 - acc: 0.4444 - val_loss: 0.8463 - val_acc: 0.2500\n",
      "Epoch 18/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.8229 - acc: 0.4444 - val_loss: 0.8384 - val_acc: 0.2500\n",
      "Epoch 19/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.8148 - acc: 0.4444 - val_loss: 0.8296 - val_acc: 0.2500\n",
      "Epoch 20/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.8069 - acc: 0.4444 - val_loss: 0.8201 - val_acc: 0.2500\n",
      "Epoch 21/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.7991 - acc: 0.4444 - val_loss: 0.8110 - val_acc: 0.2500\n",
      "Epoch 22/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.7915 - acc: 0.4444 - val_loss: 0.8025 - val_acc: 0.2500\n",
      "Epoch 23/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.7843 - acc: 0.4815 - val_loss: 0.7939 - val_acc: 0.2500\n",
      "Epoch 24/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.7774 - acc: 0.4815 - val_loss: 0.7851 - val_acc: 0.2500\n",
      "Epoch 25/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.7705 - acc: 0.4815 - val_loss: 0.7758 - val_acc: 0.2500\n",
      "Epoch 26/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.7637 - acc: 0.5185 - val_loss: 0.7659 - val_acc: 0.2500\n",
      "Epoch 27/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.7569 - acc: 0.5185 - val_loss: 0.7559 - val_acc: 0.2500\n",
      "Epoch 28/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.7501 - acc: 0.5185 - val_loss: 0.7464 - val_acc: 0.2500\n",
      "Epoch 29/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.7434 - acc: 0.5185 - val_loss: 0.7367 - val_acc: 0.2500\n",
      "Epoch 30/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.7369 - acc: 0.5185 - val_loss: 0.7268 - val_acc: 0.2500\n",
      "Epoch 31/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.7304 - acc: 0.5185 - val_loss: 0.7167 - val_acc: 0.2500\n",
      "Epoch 32/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.7240 - acc: 0.5185 - val_loss: 0.7065 - val_acc: 0.2500\n",
      "Epoch 33/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.7177 - acc: 0.5185 - val_loss: 0.6962 - val_acc: 0.5000\n",
      "Epoch 34/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.7116 - acc: 0.5926 - val_loss: 0.6856 - val_acc: 0.5000\n",
      "Epoch 35/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.7057 - acc: 0.5926 - val_loss: 0.6756 - val_acc: 0.5000\n",
      "Epoch 36/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6999 - acc: 0.6296 - val_loss: 0.6663 - val_acc: 0.5000\n",
      "Epoch 37/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.6943 - acc: 0.6296 - val_loss: 0.6572 - val_acc: 0.5000\n",
      "Epoch 38/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.6890 - acc: 0.6296 - val_loss: 0.6477 - val_acc: 0.5000\n",
      "Epoch 39/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6840 - acc: 0.6296 - val_loss: 0.6375 - val_acc: 0.5000\n",
      "Epoch 40/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6792 - acc: 0.6296 - val_loss: 0.6278 - val_acc: 0.7500\n",
      "Epoch 41/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6746 - acc: 0.6296 - val_loss: 0.6190 - val_acc: 0.7500\n",
      "Epoch 42/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6701 - acc: 0.6296 - val_loss: 0.6110 - val_acc: 0.7500\n",
      "Epoch 43/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6659 - acc: 0.6667 - val_loss: 0.6039 - val_acc: 0.7500\n",
      "Epoch 44/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6619 - acc: 0.6667 - val_loss: 0.5976 - val_acc: 0.7500\n",
      "Epoch 45/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6582 - acc: 0.7037 - val_loss: 0.5917 - val_acc: 0.7500\n",
      "Epoch 46/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.6546 - acc: 0.7037 - val_loss: 0.5865 - val_acc: 0.7500\n",
      "Epoch 47/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6511 - acc: 0.7037 - val_loss: 0.5824 - val_acc: 0.7500\n",
      "Epoch 48/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6479 - acc: 0.7037 - val_loss: 0.5787 - val_acc: 0.7500\n",
      "Epoch 49/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6450 - acc: 0.7037 - val_loss: 0.5754 - val_acc: 0.7500\n",
      "Epoch 50/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6421 - acc: 0.7037 - val_loss: 0.5727 - val_acc: 0.7500\n",
      "Epoch 51/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6392 - acc: 0.7037 - val_loss: 0.5707 - val_acc: 0.7500\n",
      "Epoch 52/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.6364 - acc: 0.7037 - val_loss: 0.5694 - val_acc: 0.7500\n",
      "Epoch 53/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6336 - acc: 0.6667 - val_loss: 0.5689 - val_acc: 0.7500\n",
      "Epoch 54/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6309 - acc: 0.6667 - val_loss: 0.5692 - val_acc: 0.7500\n",
      "Epoch 55/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6282 - acc: 0.6667 - val_loss: 0.5698 - val_acc: 0.7500\n",
      "Epoch 56/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6255 - acc: 0.6667 - val_loss: 0.5705 - val_acc: 0.7500\n",
      "Epoch 57/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6229 - acc: 0.6667 - val_loss: 0.5712 - val_acc: 0.7500\n",
      "Epoch 58/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6203 - acc: 0.6667 - val_loss: 0.5720 - val_acc: 0.7500\n",
      "Epoch 59/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6177 - acc: 0.6667 - val_loss: 0.5727 - val_acc: 0.7500\n",
      "Epoch 60/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.6151 - acc: 0.6667 - val_loss: 0.5733 - val_acc: 0.7500\n",
      "Epoch 61/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.6125 - acc: 0.6667 - val_loss: 0.5740 - val_acc: 0.7500\n",
      "Epoch 62/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.6099 - acc: 0.6667 - val_loss: 0.5745 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.6075 - acc: 0.7037 - val_loss: 0.5750 - val_acc: 0.7500\n",
      "Epoch 64/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6050 - acc: 0.7037 - val_loss: 0.5752 - val_acc: 0.7500\n",
      "Epoch 65/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6026 - acc: 0.7037 - val_loss: 0.5745 - val_acc: 0.7500\n",
      "Epoch 66/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.6003 - acc: 0.6667 - val_loss: 0.5734 - val_acc: 0.7500\n",
      "Epoch 67/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5979 - acc: 0.6667 - val_loss: 0.5717 - val_acc: 0.7500\n",
      "Epoch 68/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5956 - acc: 0.6667 - val_loss: 0.5694 - val_acc: 0.7500\n",
      "Epoch 69/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5932 - acc: 0.6667 - val_loss: 0.5668 - val_acc: 0.7500\n",
      "Epoch 70/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5908 - acc: 0.6667 - val_loss: 0.5645 - val_acc: 0.7500\n",
      "Epoch 71/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5885 - acc: 0.7037 - val_loss: 0.5620 - val_acc: 0.7500\n",
      "Epoch 72/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5863 - acc: 0.7037 - val_loss: 0.5591 - val_acc: 0.7500\n",
      "Epoch 73/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5840 - acc: 0.7037 - val_loss: 0.5560 - val_acc: 0.7500\n",
      "Epoch 74/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5817 - acc: 0.7037 - val_loss: 0.5530 - val_acc: 0.7500\n",
      "Epoch 75/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5795 - acc: 0.7037 - val_loss: 0.5509 - val_acc: 0.7500\n",
      "Epoch 76/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5774 - acc: 0.7037 - val_loss: 0.5497 - val_acc: 0.7500\n",
      "Epoch 77/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5754 - acc: 0.7037 - val_loss: 0.5491 - val_acc: 0.7500\n",
      "Epoch 78/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5733 - acc: 0.7037 - val_loss: 0.5490 - val_acc: 0.7500\n",
      "Epoch 79/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5714 - acc: 0.7037 - val_loss: 0.5491 - val_acc: 0.7500\n",
      "Epoch 80/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5695 - acc: 0.7037 - val_loss: 0.5499 - val_acc: 0.7500\n",
      "Epoch 81/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5677 - acc: 0.7037 - val_loss: 0.5512 - val_acc: 0.7500\n",
      "Epoch 82/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5659 - acc: 0.7037 - val_loss: 0.5520 - val_acc: 0.7500\n",
      "Epoch 83/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5641 - acc: 0.7037 - val_loss: 0.5524 - val_acc: 0.7500\n",
      "Epoch 84/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5624 - acc: 0.7037 - val_loss: 0.5525 - val_acc: 0.7500\n",
      "Epoch 85/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5607 - acc: 0.7037 - val_loss: 0.5523 - val_acc: 0.7500\n",
      "Epoch 86/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5590 - acc: 0.7037 - val_loss: 0.5521 - val_acc: 0.7500\n",
      "Epoch 87/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5573 - acc: 0.7037 - val_loss: 0.5525 - val_acc: 0.7500\n",
      "Epoch 88/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5557 - acc: 0.7037 - val_loss: 0.5531 - val_acc: 0.7500\n",
      "Epoch 89/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5541 - acc: 0.7037 - val_loss: 0.5539 - val_acc: 0.7500\n",
      "Epoch 90/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5525 - acc: 0.7037 - val_loss: 0.5547 - val_acc: 0.7500\n",
      "Epoch 91/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5509 - acc: 0.7037 - val_loss: 0.5556 - val_acc: 0.7500\n",
      "Epoch 92/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5493 - acc: 0.7037 - val_loss: 0.5564 - val_acc: 0.7500\n",
      "Epoch 93/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5477 - acc: 0.7037 - val_loss: 0.5570 - val_acc: 0.7500\n",
      "Epoch 94/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5462 - acc: 0.7037 - val_loss: 0.5583 - val_acc: 0.7500\n",
      "Epoch 95/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5446 - acc: 0.7037 - val_loss: 0.5590 - val_acc: 0.7500\n",
      "Epoch 96/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5430 - acc: 0.7037 - val_loss: 0.5593 - val_acc: 0.7500\n",
      "Epoch 97/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5415 - acc: 0.7037 - val_loss: 0.5591 - val_acc: 0.7500\n",
      "Epoch 98/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5399 - acc: 0.7037 - val_loss: 0.5589 - val_acc: 0.7500\n",
      "Epoch 99/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5384 - acc: 0.7037 - val_loss: 0.5586 - val_acc: 0.7500\n",
      "Epoch 100/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5368 - acc: 0.7037 - val_loss: 0.5578 - val_acc: 0.7500\n",
      "Epoch 101/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5353 - acc: 0.7037 - val_loss: 0.5577 - val_acc: 0.7500\n",
      "Epoch 102/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5338 - acc: 0.7037 - val_loss: 0.5581 - val_acc: 0.7500\n",
      "Epoch 103/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5322 - acc: 0.7037 - val_loss: 0.5585 - val_acc: 0.7500\n",
      "Epoch 104/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5309 - acc: 0.7037 - val_loss: 0.5583 - val_acc: 0.7500\n",
      "Epoch 105/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5296 - acc: 0.7037 - val_loss: 0.5579 - val_acc: 0.7500\n",
      "Epoch 106/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5282 - acc: 0.7037 - val_loss: 0.5571 - val_acc: 0.7500\n",
      "Epoch 107/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5268 - acc: 0.7037 - val_loss: 0.5565 - val_acc: 0.7500\n",
      "Epoch 108/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5254 - acc: 0.7037 - val_loss: 0.5555 - val_acc: 0.7500\n",
      "Epoch 109/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5239 - acc: 0.7037 - val_loss: 0.5544 - val_acc: 0.7500\n",
      "Epoch 110/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5226 - acc: 0.7037 - val_loss: 0.5537 - val_acc: 0.7500\n",
      "Epoch 111/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5212 - acc: 0.7037 - val_loss: 0.5540 - val_acc: 0.7500\n",
      "Epoch 112/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5199 - acc: 0.7037 - val_loss: 0.5550 - val_acc: 0.7500\n",
      "Epoch 113/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5185 - acc: 0.7037 - val_loss: 0.5557 - val_acc: 0.7500\n",
      "Epoch 114/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5172 - acc: 0.7037 - val_loss: 0.5569 - val_acc: 0.7500\n",
      "Epoch 115/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5158 - acc: 0.7037 - val_loss: 0.5578 - val_acc: 0.7500\n",
      "Epoch 116/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5145 - acc: 0.7037 - val_loss: 0.5585 - val_acc: 0.7500\n",
      "Epoch 117/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5131 - acc: 0.7037 - val_loss: 0.5585 - val_acc: 0.7500\n",
      "Epoch 118/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5119 - acc: 0.7037 - val_loss: 0.5591 - val_acc: 0.7500\n",
      "Epoch 119/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5106 - acc: 0.7037 - val_loss: 0.5599 - val_acc: 0.7500\n",
      "Epoch 120/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5092 - acc: 0.7037 - val_loss: 0.5609 - val_acc: 0.7500\n",
      "Epoch 121/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5080 - acc: 0.7037 - val_loss: 0.5614 - val_acc: 0.7500\n",
      "Epoch 122/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.5066 - acc: 0.7037 - val_loss: 0.5615 - val_acc: 0.7500\n",
      "Epoch 123/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5052 - acc: 0.7037 - val_loss: 0.5617 - val_acc: 0.7500\n",
      "Epoch 124/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5039 - acc: 0.7037 - val_loss: 0.5624 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5027 - acc: 0.7037 - val_loss: 0.5642 - val_acc: 0.7500\n",
      "Epoch 126/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5014 - acc: 0.7037 - val_loss: 0.5667 - val_acc: 0.7500\n",
      "Epoch 127/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.5001 - acc: 0.7037 - val_loss: 0.5692 - val_acc: 0.7500\n",
      "Epoch 128/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4987 - acc: 0.7037 - val_loss: 0.5709 - val_acc: 0.7500\n",
      "Epoch 129/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4975 - acc: 0.7407 - val_loss: 0.5718 - val_acc: 0.7500\n",
      "Epoch 130/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4962 - acc: 0.7407 - val_loss: 0.5727 - val_acc: 0.7500\n",
      "Epoch 131/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4949 - acc: 0.7407 - val_loss: 0.5732 - val_acc: 0.7500\n",
      "Epoch 132/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4937 - acc: 0.7407 - val_loss: 0.5743 - val_acc: 0.7500\n",
      "Epoch 133/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4924 - acc: 0.7407 - val_loss: 0.5759 - val_acc: 0.7500\n",
      "Epoch 134/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4912 - acc: 0.7407 - val_loss: 0.5782 - val_acc: 0.7500\n",
      "Epoch 135/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4899 - acc: 0.7407 - val_loss: 0.5809 - val_acc: 0.7500\n",
      "Epoch 136/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4886 - acc: 0.7407 - val_loss: 0.5835 - val_acc: 0.7500\n",
      "Epoch 137/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4874 - acc: 0.7407 - val_loss: 0.5850 - val_acc: 0.7500\n",
      "Epoch 138/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4861 - acc: 0.7407 - val_loss: 0.5863 - val_acc: 0.7500\n",
      "Epoch 139/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4849 - acc: 0.7407 - val_loss: 0.5873 - val_acc: 0.7500\n",
      "Epoch 140/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4836 - acc: 0.7407 - val_loss: 0.5882 - val_acc: 0.7500\n",
      "Epoch 141/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4823 - acc: 0.7407 - val_loss: 0.5892 - val_acc: 0.7500\n",
      "Epoch 142/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4810 - acc: 0.7407 - val_loss: 0.5911 - val_acc: 0.7500\n",
      "Epoch 143/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4798 - acc: 0.7407 - val_loss: 0.5921 - val_acc: 0.7500\n",
      "Epoch 144/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4785 - acc: 0.7778 - val_loss: 0.5925 - val_acc: 0.7500\n",
      "Epoch 145/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4772 - acc: 0.7778 - val_loss: 0.5927 - val_acc: 0.7500\n",
      "Epoch 146/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4760 - acc: 0.7778 - val_loss: 0.5934 - val_acc: 0.7500\n",
      "Epoch 147/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4748 - acc: 0.7778 - val_loss: 0.5955 - val_acc: 0.7500\n",
      "Epoch 148/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4735 - acc: 0.7778 - val_loss: 0.5972 - val_acc: 0.7500\n",
      "Epoch 149/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4723 - acc: 0.7778 - val_loss: 0.5994 - val_acc: 0.7500\n",
      "Epoch 150/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4711 - acc: 0.7778 - val_loss: 0.6023 - val_acc: 0.7500\n",
      "Epoch 151/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4699 - acc: 0.7778 - val_loss: 0.6045 - val_acc: 0.7500\n",
      "Epoch 152/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4687 - acc: 0.7778 - val_loss: 0.6067 - val_acc: 0.7500\n",
      "Epoch 153/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.4675 - acc: 0.7778 - val_loss: 0.6086 - val_acc: 0.7500\n",
      "Epoch 154/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4663 - acc: 0.7778 - val_loss: 0.6109 - val_acc: 0.7500\n",
      "Epoch 155/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4651 - acc: 0.7778 - val_loss: 0.6124 - val_acc: 0.7500\n",
      "Epoch 156/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4639 - acc: 0.7778 - val_loss: 0.6137 - val_acc: 0.7500\n",
      "Epoch 157/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4627 - acc: 0.7778 - val_loss: 0.6156 - val_acc: 0.7500\n",
      "Epoch 158/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4616 - acc: 0.7778 - val_loss: 0.6184 - val_acc: 0.7500\n",
      "Epoch 159/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4603 - acc: 0.7778 - val_loss: 0.6206 - val_acc: 0.7500\n",
      "Epoch 160/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4592 - acc: 0.7778 - val_loss: 0.6221 - val_acc: 0.7500\n",
      "Epoch 161/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4581 - acc: 0.7778 - val_loss: 0.6242 - val_acc: 0.7500\n",
      "Epoch 162/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4568 - acc: 0.7778 - val_loss: 0.6270 - val_acc: 0.7500\n",
      "Epoch 163/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4558 - acc: 0.7778 - val_loss: 0.6304 - val_acc: 0.7500\n",
      "Epoch 164/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4546 - acc: 0.7778 - val_loss: 0.6338 - val_acc: 0.7500\n",
      "Epoch 165/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4534 - acc: 0.7778 - val_loss: 0.6362 - val_acc: 0.7500\n",
      "Epoch 166/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4524 - acc: 0.7778 - val_loss: 0.6372 - val_acc: 0.7500\n",
      "Epoch 167/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4511 - acc: 0.7778 - val_loss: 0.6378 - val_acc: 0.7500\n",
      "Epoch 168/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4499 - acc: 0.7778 - val_loss: 0.6389 - val_acc: 0.7500\n",
      "Epoch 169/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4488 - acc: 0.7778 - val_loss: 0.6415 - val_acc: 0.7500\n",
      "Epoch 170/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4477 - acc: 0.7778 - val_loss: 0.6446 - val_acc: 0.7500\n",
      "Epoch 171/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4465 - acc: 0.7778 - val_loss: 0.6480 - val_acc: 0.7500\n",
      "Epoch 172/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4453 - acc: 0.7778 - val_loss: 0.6506 - val_acc: 0.7500\n",
      "Epoch 173/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4442 - acc: 0.7778 - val_loss: 0.6534 - val_acc: 0.7500\n",
      "Epoch 174/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4430 - acc: 0.7778 - val_loss: 0.6563 - val_acc: 0.7500\n",
      "Epoch 175/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4418 - acc: 0.7778 - val_loss: 0.6587 - val_acc: 0.7500\n",
      "Epoch 176/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4407 - acc: 0.7778 - val_loss: 0.6611 - val_acc: 0.7500\n",
      "Epoch 177/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4394 - acc: 0.7778 - val_loss: 0.6652 - val_acc: 0.7500\n",
      "Epoch 178/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4381 - acc: 0.7778 - val_loss: 0.6700 - val_acc: 0.7500\n",
      "Epoch 179/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4369 - acc: 0.7778 - val_loss: 0.6750 - val_acc: 0.7500\n",
      "Epoch 180/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4358 - acc: 0.7778 - val_loss: 0.6783 - val_acc: 0.7500\n",
      "Epoch 181/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4346 - acc: 0.7778 - val_loss: 0.6799 - val_acc: 0.7500\n",
      "Epoch 182/200\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.4334 - acc: 0.7778 - val_loss: 0.6827 - val_acc: 0.7500\n",
      "Epoch 183/200\n",
      "27/27 [==============================] - 0s 833us/step - loss: 0.4323 - acc: 0.7778 - val_loss: 0.6855 - val_acc: 0.7500\n",
      "Epoch 184/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4312 - acc: 0.7778 - val_loss: 0.6886 - val_acc: 0.7500\n",
      "Epoch 185/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4301 - acc: 0.7778 - val_loss: 0.6916 - val_acc: 0.7500\n",
      "Epoch 186/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4288 - acc: 0.7778 - val_loss: 0.6936 - val_acc: 0.7500\n",
      "Epoch 187/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4276 - acc: 0.7778 - val_loss: 0.6929 - val_acc: 0.7500\n",
      "Epoch 188/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4267 - acc: 0.7778 - val_loss: 0.7002 - val_acc: 0.7500\n",
      "Epoch 189/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4252 - acc: 0.7778 - val_loss: 0.7067 - val_acc: 0.7500\n",
      "Epoch 190/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4241 - acc: 0.7778 - val_loss: 0.7123 - val_acc: 0.7500\n",
      "Epoch 191/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4229 - acc: 0.7778 - val_loss: 0.7165 - val_acc: 0.7500\n",
      "Epoch 192/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4216 - acc: 0.7778 - val_loss: 0.7160 - val_acc: 0.7500\n",
      "Epoch 193/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4208 - acc: 0.7778 - val_loss: 0.7216 - val_acc: 0.7500\n",
      "Epoch 194/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4195 - acc: 0.7778 - val_loss: 0.7304 - val_acc: 0.7500\n",
      "Epoch 195/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4182 - acc: 0.7778 - val_loss: 0.7347 - val_acc: 0.7500\n",
      "Epoch 196/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4172 - acc: 0.7778 - val_loss: 0.7360 - val_acc: 0.7500\n",
      "Epoch 197/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4158 - acc: 0.7778 - val_loss: 0.7365 - val_acc: 0.7500\n",
      "Epoch 198/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4146 - acc: 0.7778 - val_loss: 0.7369 - val_acc: 0.7500\n",
      "Epoch 199/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4135 - acc: 0.7778 - val_loss: 0.7426 - val_acc: 0.7500\n",
      "Epoch 200/200\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.4122 - acc: 0.7778 - val_loss: 0.7503 - val_acc: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x43f86c50>"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Standard model\n",
    "Standard_model = Sequential()\n",
    "Standard_model.add(Dense(24, input_dim=2, activation='relu'))\n",
    "Standard_model.add(Dense(48, activation='relu'))\n",
    "Standard_model.add(Dense(62, activation='relu'))\n",
    "Standard_model.add(Dense(8, activation='relu'))\n",
    "Standard_model.add(Dense(3, activation='linear'))\n",
    "Standard_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "Standard_model.fit(train_features_Standard,\n",
    "                   train_labels_Standard,\n",
    "                   validation_data=(test_features_Standard, test_labels_Standard),\n",
    "                   batch_size=30,\n",
    "                   epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax Prediction:\n",
      "\n",
      " [[1100.2214       -0.02532227    0.5947546 ]\n",
      " [1100.7401        1.1691506     1.4559562 ]\n",
      " [1100.8046        0.9829279     1.2486337 ]\n",
      " [1100.811         0.9336802     1.2230921 ]]\n",
      "\n",
      "Actual:\n",
      "\n",
      " [[1101.            0.05          0.4       ]\n",
      " [1100.16666667    1.05          1.4       ]\n",
      " [1100.5           0.85          1.28235294]\n",
      " [1100.            0.75          1.30196078]]\n"
     ]
    }
   ],
   "source": [
    "# MinMax prediction on testing\n",
    "prediction = MinMax_model.predict(test_features_MinMax)\n",
    "prediction = MinMax_scaler.inverse_transform(prediction)\n",
    "# compare\n",
    "print('MinMax Prediction:\\n\\n', prediction)\n",
    "print('\\nActual:\\n\\n', MinMax_scaler.inverse_transform(test_labels_MinMax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax Prediction:\n",
      "\n",
      " [[1100.2214       -0.02532221    0.5947544 ]]\n",
      "\n",
      "Actual:\n",
      "\n",
      " [[1100.      0.05    0.4 ]]\n"
     ]
    }
   ],
   "source": [
    "# MinMax prediction on validation\n",
    "prediction = MinMax_model.predict(validation_features_MinMax)\n",
    "prediction = MinMax_scaler.inverse_transform(prediction)\n",
    "# compare\n",
    "print('MinMax Prediction:\\n\\n', prediction)\n",
    "print('\\nActual:\\n\\n', MinMax_scaler.inverse_transform(validation_labels_MinMax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Prediction:\n",
      "\n",
      " [[1099.858        -1.7899023    -0.01922235]\n",
      " [1100.3661        1.1970179     0.38480955]\n",
      " [1100.2012        0.852707      0.19228794]\n",
      " [1100.1919        0.7170883     0.26936454]]\n",
      "\n",
      "Actual:\n",
      "\n",
      " [[1101.52752523   -1.60930828   -1.32149669]\n",
      " [1099.34534633    1.04558497    1.1516394 ]\n",
      " [1100.21821789    0.51460632    0.86068221]\n",
      " [1098.90891055    0.24911699    0.90917508]]\n"
     ]
    }
   ],
   "source": [
    "# Standard prediction on testing\n",
    "prediction = Standard_model.predict(test_features_Standard)\n",
    "prediction = Standard_scaler.inverse_transform(prediction)\n",
    "# compare\n",
    "print('Standard Prediction:\\n\\n', prediction)\n",
    "print('\\nActual:\\n\\n', Standard_scaler.inverse_transform(test_labels_Standard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spindle_speed(rpm)</th>\n",
       "      <th>Feed_rate(mm/rev)</th>\n",
       "      <th>Depth_of_cut(mm)</th>\n",
       "      <th>MRR(mm3/min)</th>\n",
       "      <th>Ra(µm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1100</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.69</td>\n",
       "      <td>2020.44</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1095</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.20</td>\n",
       "      <td>6054.12</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1097</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.14</td>\n",
       "      <td>5329.21</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1094</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.15</td>\n",
       "      <td>5197.29</td>\n",
       "      <td>1.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Spindle_speed(rpm)  Feed_rate(mm/rev)  Depth_of_cut(mm)  MRR(mm3/min)  \\\n",
       "1                1100               0.05              0.69       2020.44   \n",
       "2                1095               0.15              1.20       6054.12   \n",
       "3                1097               0.13              1.14       5329.21   \n",
       "4                1094               0.12              1.15       5197.29   \n",
       "\n",
       "   Ra(µm)  \n",
       "1    0.29  \n",
       "2    1.87  \n",
       "3    1.54  \n",
       "4    1.45  "
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Prediction:\n",
      "\n",
      " [[1100.1439        0.48441344    0.12105619]]\n",
      "\n",
      "Actual:\n",
      "\n",
      " [[1100.      0.05    0.4 ]]\n"
     ]
    }
   ],
   "source": [
    "# Standard prediction on validation\n",
    "prediction = Standard_model.predict(validation_features_Standard)\n",
    "prediction = Standard_scaler.inverse_transform(prediction)\n",
    "# compare\n",
    "print('Standard Prediction:\\n\\n', prediction)\n",
    "print('\\nActual:\\n\\n', Standard_scaler.inverse_transform(validation_labels_Standard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spindle_speed(rpm)    1100.00\n",
       "Feed_rate(mm/rev)        0.05\n",
       "Depth_of_cut(mm)         0.40\n",
       "MRR(mm3/min)          1106.51\n",
       "Ra(µm)                   0.21\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
